#!/bin/bash
#SBATCH --job-name="setest-spark"
#SBATCH --output="setest-spark.%j.%N.out"
#SBATCH --partition=compute
#SBATCH --ntasks-per-node=24
#SBATCH -t 00:30:00
export MODULEPATH=/share/apps/compute/modulefiles/applications:$MODULEPATH
module load rdma-spark/0.9.1 
export PATH=/opt/hadoop/2.6.0/sbin:$PATH
export HADOOP_CONF_DIR=$HOME/cometcluster
export WORKDIR=`pwd`
myhadoop-configure.sh

# Remove old Result
rm $WORKDIR/results/Result$1-$2

#start-all.sh
start-dfs.sh
#start-yarn.sh

source $HADOOP_CONF_DIR/spark/spark-env.sh

myspark start

spark-submit --class ReduceMicroBenchmark --total-executor-cores $3 --executor-cores $2 --executor-memory 7g --driver-memory 7g  hpcfbd-spark-1.0.jar $WORKDIR/results/Result$1-$2

myspark stop
#stop-yarn.sh
stop-dfs.sh
#stop-all.sh

myhadoop-cleanup.sh
