#!/bin/bash
#SBATCH --job-name="setest-spark"
#SBATCH --output="setest-spark.%j.%N.out"
#SBATCH --partition=compute
#SBATCH --ntasks-per-node=24
#SBATCH -t 00:30:00
export HADOOP_CONF_DIR=/home/$USER/cometcluster
export WORKDIR=`pwd`
module load hadoop
module load spark
myhadoop-configure.sh
export PATH=/opt/hadoop/2.6.0/sbin:$PATH

# Remove old Result
rm $WORKDIR/results/Result$1-$2

#start-all.sh
start-dfs.sh
start-yarn.sh


spark-submit --class ReduceMicroBenchmark  --master yarn-cluster --num-executors $1 --executor-cores $2 --executor-memory 7g --driver-memory 7g hpcfbd-spark-1.0.jar $WORKDIR/results/Result$1-$2


stop-yarn.sh
stop-dfs.sh
#stop-all.sh

myhadoop-cleanup.sh
