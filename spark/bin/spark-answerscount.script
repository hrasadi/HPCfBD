#!/bin/bash
#SBATCH --job-name="setest-spark"
#SBATCH --output="setest-spark.%j.%N.out"
#SBATCH --partition=compute
#SBATCH --ntasks-per-node=24
#SBATCH -t 00:30:00

#HOW TO USE
# sbatch --nodes [num_nodes] spark.script [num_nodes] [num_cores_per_node] [input_filename]
# outputs will be in 'results/Result[num_nodes]_[num_cores_per_node]'


export HADOOP_CONF_DIR=/home/$USER/cometcluster
export WORKDIR=`pwd`
module load hadoop
module load spark
myhadoop-configure.sh
export PATH=/opt/hadoop/2.6.0/sbin:$PATH

# Remove old Result
rm $WORKDIR/results/Result$1-$2

#start-all.sh
start-dfs.sh
start-yarn.sh

hdfs dfs -mkdir -p /user/$USER/input
echo Putting input file from $WORKDIR/$3 to HDFS
hdfs dfs -put $WORKDIR/$3 /user/$USER/input/$3

spark-submit --class AnsersCountBenchmark --master yarn-cluster --num-executors $1 --executor-cores $2 --executor-memory 6g hpcfbd-spark-1.0.jar /user/$USER/input/$3 $WORKDIR/results/Result$1-$2


stop-yarn.sh
stop-dfs.sh
#stop-all.sh

myhadoop-cleanup.sh
